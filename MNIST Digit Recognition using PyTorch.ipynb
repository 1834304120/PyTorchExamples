{"cells":[{"metadata":{"_uuid":"65670791be0f9dbdabacec767b7aa930f88f57cb"},"cell_type":"markdown","source":"#### Imports"},{"metadata":{"trusted":true,"_uuid":"8b92df54d984e6e40ada9af22e0212e435dbf783"},"cell_type":"code","source":"#pytorch utility imports\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#neural net imports\nimport torch.nn as nn, torch.nn.functional as F, torch.optim as optim\nfrom torch.autograd import Variable\n\n#import external libraries\nimport pandas as pd,numpy as np,matplotlib.pyplot as plt, os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n%matplotlib inline\n\n#Set device to GPU or CPU based on availability\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')","execution_count":68,"outputs":[]},{"metadata":{"_uuid":"3f5ceccdfd524d9e51c2670a58449ce49e12363a"},"cell_type":"markdown","source":"#### Load Data "},{"metadata":{"trusted":true,"_uuid":"68ff1845fdc154f7dcd978ebf33e599d675ad26b"},"cell_type":"code","source":"input_folder_path = \"../input/\"\n\n#The CSV contains a flat file of images, i.e. each 28*28 image is flattened into a row of 784 colums (1 column represents a pixel value)\n#For CNN, we would need to reshape this to our desired shape\ntrain_df = pd.read_csv(input_folder_path+\"train.csv\")\n\n#First column is the target/label\ntrain_labels = train_df['label'].values\n#Pixels values start from the 2nd column\ntrain_images = (train_df.iloc[:,1:].values).astype('float32')\n\n#Training and Validation Split\ntrain_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels,\n                                                                     stratify=train_labels, random_state=2020,\n                                                                     test_size=0.2)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a0467a07f79074cb03dfafcf70a989a1de654e8e"},"cell_type":"code","source":"#Here we reshape the flat row into [#images,#Channels,#Width,#Height]\n#Given this a simple grayscale image, we will have just 1 channel\ntrain_images = train_images.reshape(train_images.shape[0],1,28, 28)\nval_images = val_images.reshape(val_images.shape[0],1,28, 28)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"f5cc49c6ced3bbacfc4f2dbdfab19471ec4ddb04"},"cell_type":"markdown","source":"#### Let's plot few images to see their values"},{"metadata":{"trusted":true,"_uuid":"1de567ffcf179220917f3344066454f32a6127bb"},"cell_type":"code","source":"#train samples\nfor i in range(0, 6):\n    plt.subplot(160 + (i+1))\n    plt.imshow(train_images[i].reshape(28,28), cmap=plt.get_cmap('gray'))\n    plt.title(train_labels[i])","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 6 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"_uuid":"fa81fe0a91f7894e144d09b9268d67a6773c51b6"},"cell_type":"markdown","source":"#### Convert images to tensors\nNormalize the images too"},{"metadata":{"trusted":true,"_uuid":"d39d3fb5b0613fabf72d4a17ddf638bf4ad28700"},"cell_type":"code","source":"#Covert Train Images from pandas/numpy to tensor and normalize the values\ntrain_images_tensor = torch.tensor(train_images)/255.0\ntrain_images_tensor = train_images_tensor.view(-1,1,28,28)\ntrain_labels_tensor = torch.tensor(train_labels)\n#Create a train TensorDataset\ntrain_tensor = TensorDataset(train_images_tensor, train_labels_tensor)\n\n#Covert Validation Images from pandas/numpy to tensor and normalize the values\nval_images_tensor = torch.tensor(val_images)/255.0\nval_images_tensor = val_images_tensor.view(-1,1,28,28)\nval_labels_tensor = torch.tensor(val_labels)\n#Create a Validation TensorDataset\nval_tensor = TensorDataset(val_images_tensor, val_labels_tensor)\n\nprint(\"Train Labels Shape:\",train_labels_tensor.shape)\nprint(\"Train Images Shape:\",train_images_tensor.shape)\nprint(\"Validation Labels Shape:\",val_labels_tensor.shape)\nprint(\"Validation Images Shape:\",val_images_tensor.shape)\n\n#Load Train and Validation TensorDatasets into the data generator for Training iterations\ntrain_loader = DataLoader(train_tensor, batch_size=64, num_workers=2, shuffle=True)\nval_loader = DataLoader(val_tensor, batch_size=64, num_workers=2, shuffle=True)\n","execution_count":61,"outputs":[{"output_type":"stream","text":"Train Labels Shape: torch.Size([33600])\nTrain Images Shape: torch.Size([33600, 1, 28, 28])\nValidation Labels Shape: torch.Size([8400])\nValidation Images Shape: torch.Size([8400, 1, 28, 28])\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"f99f8c9b1bfe0474f27b72005e7708b6d9fd091a"},"cell_type":"markdown","source":"#### Define the CNN Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"class ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        #First unit of convolution\n        self.conv_unit_1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        #Second unit of convolution        \n        self.conv_unit_2 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        #FFully connected layers\n        self.fc1 = nn.Linear(7*7*32, 128)       \n        self.fc2 = nn.Linear(128, 10)        \n        \n    def forward(self, x):       \n        out = self.conv_unit_1(x)\n        out = self.conv_unit_2(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc1(out)\n        out = self.fc2(out)        \n        out = F.log_softmax(out,dim=1)                                \n        return out\n\n    \n    \n#Define Functions for Model Evaluation and generating Predictions    \ndef make_predictions(data_loader):\n    model.eval()\n    test_preds = torch.LongTensor()\n    actual = torch.LongTensor()\n    \n    for data, target in data_loader:\n        \n        if torch.cuda.is_available():\n            data = data.cuda()\n        output = model(data)\n        \n        preds = output.cpu().data.max(1, keepdim=True)[1]\n        test_preds = torch.cat((test_preds, preds), dim=0)\n        actual  = torch.cat((actual,target),dim=0)\n        \n    return actual,test_preds\n\ndef evaluate(data_loader):\n    model.eval()\n    loss = 0\n    correct = 0\n    \n    for data, target in data_loader:        \n        if torch.cuda.is_available():\n            data = data.cuda()\n            target = target.cuda()\n        output = model(data)\n        loss += F.cross_entropy(output, target, size_average=False).data.item()\n        predicted = output.data.max(1, keepdim=True)[1]   \n        correct += (target.reshape(-1,1) == predicted.reshape(-1,1)).float().sum()        \n        \n    loss /= len(data_loader.dataset)\n        \n    print('\\nAverage Val Loss: {:.4f}, Val Accuracy: {}/{} ({:.3f}%)\\n'.format(\n        loss, correct, len(data_loader.dataset),\n        100. * correct / len(data_loader.dataset)))    \n#Create Model    \nmodel = ConvNet(10).to(device)\n\n#Define Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)    \nprint(model)\n","execution_count":63,"outputs":[{"output_type":"stream","text":"ConvNet(\n  (conv_unit_1): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_unit_2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc1): Linear(in_features=1568, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"ff6cd03cb966c7f9c728f014c74edf87546c3d54"},"cell_type":"code","source":"","execution_count":64,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 5\n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()                \n    #After each epoch print Train loss and validation loss + accuracy\n    print ('Epoch [{}/{}], Loss: {:.4f}' .format(epoch+1, num_epochs, loss.item()))\n    evaluate(val_loader)","execution_count":65,"outputs":[{"output_type":"stream","text":"Epoch [1/5], Loss: 0.0512\n\nAverage Val Loss: 0.0631, Val Accuracy: 8233.0/8400 (98.012%)\n\nEpoch [2/5], Loss: 0.0320\n\nAverage Val Loss: 0.0490, Val Accuracy: 8258.0/8400 (98.310%)\n\nEpoch [3/5], Loss: 0.0341\n\nAverage Val Loss: 0.0509, Val Accuracy: 8255.0/8400 (98.274%)\n\nEpoch [4/5], Loss: 0.1300\n\nAverage Val Loss: 0.0464, Val Accuracy: 8270.0/8400 (98.452%)\n\nEpoch [5/5], Loss: 0.0071\n\nAverage Val Loss: 0.0389, Val Accuracy: 8309.0/8400 (98.917%)\n\n","name":"stdout"}]},{"metadata":{"_uuid":"fee45fa16f75c08e1a9c628d88bb9d3176ee3a79"},"cell_type":"markdown","source":"#### Make predictions on the test set"},{"metadata":{"trusted":true,"_uuid":"5b624de03cbfef7b1e11f1bcf0efa6721c8112bd"},"cell_type":"code","source":"#Make Predictions on Validation Dataset\n\nactual, predicted = make_predictions(val_loader)\nactual,predicted = np.array(actual).reshape(-1,1),np.array(predicted).reshape(-1,1)\n\nprint(\"Validation Accuracy-\",round(accuracy_score(actual,predicted),4)*100)\nprint(confusion_matrix(actual,predicted))","execution_count":76,"outputs":[{"output_type":"stream","text":"Validation Accuracy- 98.92\n[[826   0   0   0   0   0   0   0   0   1]\n [  0 934   1   0   0   0   0   2   0   0]\n [  0   3 827   0   0   0   0   4   1   0]\n [  0   1   2 866   0   1   0   0   0   0]\n [  0   0   0   0 805   0   1   0   0   8]\n [  0   0   0   4   1 751   1   0   2   0]\n [  6   1   1   0   3   0 816   0   0   0]\n [  0   2   8   0   1   0   0 868   0   1]\n [  1   4   6   1   0   1   1   1 798   0]\n [  3   0   0   2   3   2   0   6   4 818]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}