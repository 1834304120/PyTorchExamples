{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: torch.Size([4800, 32])\n",
      "x_test.shape: torch.Size([1200, 32])\n",
      "Y_train.shape: torch.Size([4800, 1])\n",
      "y_test.shape: torch.Size([1200, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch as tch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Create 500 observations with randn | This will be tagged as 0\n",
    "X1 = tch.randn(3000, 32)\n",
    "\n",
    "#Create another 500 observations with randn slightly different from X1| This will be tagged as 0\n",
    "X2 = tch.randn(3000, 32) + 0.5\n",
    "\n",
    "#Ccombined X1 and X2\n",
    "X = tch.cat([X1, X2], dim=0)\n",
    "\n",
    "#Create 1000 Y combined 50% 0's and 50% 1's\n",
    "Y1 = tch.zeros(3000, 1)\n",
    "Y2 = tch.ones(3000, 1)\n",
    "Y = tch.cat([Y1, Y2], dim=0)\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "batch_size = 16\n",
    "validation_split = 0.2 # 20%\n",
    "random_seed= 2019\n",
    "\n",
    "#Shuffle indices\n",
    "dataset_size = X.shape[0]\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "#Create train and validation indices\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "#Create train and validation dataset\n",
    "X_train, x_test = X[train_indices], X[val_indices]\n",
    "Y_train, y_test = Y[train_indices], Y[val_indices]\n",
    "\n",
    "#Print shapes of each dataset\n",
    "print(\"X_train.shape:\",X_train.shape)\n",
    "print(\"x_test.shape:\",x_test.shape)\n",
    "print(\"Y_train.shape:\",Y_train.shape)\n",
    "print(\"y_test.shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a neural network with 2 hidden layers and 1 output layer\n",
    "#Hidden Layers will have 64 and 256 neurons\n",
    "#Output layers will have 1 neuron\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.out = nn.Linear(256, 1)\n",
    "        self.final = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        op = self.fc1(x)\n",
    "        op = self.relu1(op)        \n",
    "        op = self.fc2(op)\n",
    "        op = self.relu2(op)\n",
    "        op = self.out(op)\n",
    "        y = self.final(op)\n",
    "        return y\n",
    "    \n",
    "\n",
    "model = NeuralNetwork()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = tch.optim.Adam(model.parameters(),lr= 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - Loss:0.2551\n",
      "Epoch: 2 - Loss:0.1880\n",
      "Epoch: 3 - Loss:0.1766\n",
      "Epoch: 4 - Loss:0.1671\n",
      "Epoch: 5 - Loss:0.1581\n",
      "Epoch: 6 - Loss:0.1489\n",
      "Epoch: 7 - Loss:0.1394\n",
      "Epoch: 8 - Loss:0.1295\n",
      "Epoch: 9 - Loss:0.1193\n",
      "Epoch: 10 - Loss:0.1092\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size=16\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss= 0.0\n",
    "\n",
    "    #Explicitly start model training\n",
    "    model.train()\n",
    "    \n",
    "    for i in range(0,X_train.shape[0],batch_size):\n",
    "\n",
    "        #Extract train batch from X and Y\n",
    "        input_data = X_train[i:min(X_train.shape[0],i+batch_size)]\n",
    "        labels = Y_train[i:min(X_train.shape[0],i+batch_size)]\n",
    "        \n",
    "        #set the gradients to zero before starting to do backpropragation \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass\n",
    "        output_data  = model(input_data)\n",
    "        \n",
    "        #Caculate loss\n",
    "        loss = loss_function(output_data, labels)\n",
    "        \n",
    "        #Backpropogate\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * batch_size\n",
    "\n",
    "    print(\"Epoch: {} - Loss:{:.4f}\".format(epoch+1,train_loss/X_train.shape[0] ))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[572,  68],\n",
       "       [ 47, 513]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict\n",
    "y_test_pred = model(x_test)\n",
    "a =np.where(y_test_pred>0.5,1,0)\n",
    "confusion_matrix(y_test,a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
